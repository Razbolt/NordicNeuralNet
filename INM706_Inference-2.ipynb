{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction"
      ],
      "metadata": {
        "id": "rdfaUX3xyyB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To access the model paths first you need to mount google drive and download paths for Sequence to Sequence and T5 fine tuned version.\n",
        "\n",
        "- Please take the models from following path : https://drive.google.com/drive/folders/1XEv-_yZOl5qXl7xok6pib9GE8mm2X94-?usp=share_link\n",
        "\n",
        "- And remember to change the paths based on your folder !"
      ],
      "metadata": {
        "id": "ke8Gqb_Hy2ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from transformers import MarianTokenizer\n",
        "from transformers import T5ForConditionalGeneration,MarianTokenizer, T5Tokenizer, AdamW\n"
      ],
      "metadata": {
        "id": "5bFgecYXzq1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPy-tTGuy2NB",
        "outputId": "a30eeaab-6012-435e-af32-75a1a236ce99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7Nj3YIGyEYT",
        "outputId": "72248ca5-f9a9-4c2b-a87f-eca23cf629a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data  t5-model.pth\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "print ('Device set to {0}'.format(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnFOkwdJzaol",
        "outputId": "805602e6-2787-4ee0-efa2-c36772d977ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device set to cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Seq2Seq Architecture"
      ],
      "metadata": {
        "id": "lRKvMCfu0mMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 20\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout, batch_first=True,bidirectional=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        output, (hidden,cell) = self.rnn(embedded)\n",
        "\n",
        "        return  (hidden,cell)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, embedding_size, hidden_size, num_layers, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout, batch_first=True, bidirectional=False)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden,cell):\n",
        "        #print('----Decoder----')\n",
        "\n",
        "        x = x.unsqueeze(1) #Shape is changed to [batch_size, 1, embedding_size]\n",
        "        #print(\"Input shape:\", x.shape)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        #print(\"Embedded shape:\", embedded.shape)\n",
        "\n",
        "\n",
        "        output, (hidden,cell) = self.rnn(embedded, (hidden,cell))\n",
        "        output = output.squeeze(1)\n",
        "        #print(\"Output shape from LSTM:\", output.shape)\n",
        "\n",
        "        predictions = self.fc(output)\n",
        "\n",
        "        return predictions, hidden,cell\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, source, target=None, teacher_forcing_ratio=0.5, max_length=20):\n",
        "        # Encode the source sentence\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        # Prepare the output tensor\n",
        "        batch_size = source.shape[0]\n",
        "        target_vocab_size = self.decoder.fc.out_features\n",
        "        outputs = torch.zeros(batch_size, max_length, target_vocab_size).to(self.device)\n",
        "\n",
        "        # <sos> token is used as the initial input to the decoder\n",
        "        decoder_input = torch.zeros(batch_size, dtype=torch.long).to(self.device)  # Assuming 0 is the <sos> token index\n",
        "\n",
        "        for t in range(1, max_length):\n",
        "            decoder_output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
        "            outputs[:, t, :] = decoder_output\n",
        "            top1 = decoder_output.argmax(1)\n",
        "            decoder_input = top1\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "xbK1pArYz2Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-sv')\n",
        "\n",
        "num_token_id = tokenizer.convert_tokens_to_ids('<num>')\n",
        "if num_token_id == tokenizer.unk_token_id:\n",
        "    tokenizer.add_tokens(['<num>'])\n",
        "#Inlcude bos id\n",
        "if tokenizer.bos_token_id is None:\n",
        "    tokenizer.add_special_tokens({'bos_token': '<s>'})\n",
        "\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "print(\"Updated tokenizer vocab size:\", vocab_size) #This one should be used\n",
        "\n",
        "\n",
        "model_dict = torch.load('drive/MyDrive/s_models/seq-model.pth') # This is the place need to changed !\n",
        "\n",
        "encoder = Encoder(vocab_size,300,1024,2,0.5)\n",
        "decoder = Decoder(vocab_size,300,1024,2,0.5)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "model.load_state_dict(model_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpEOkfPT1Pct",
        "outputId": "cfd7a58c-a762-4ab4-ade7-e53efa9f9d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated tokenizer vocab size: 56436\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. 1 Inference of Seq2Seq model"
      ],
      "metadata": {
        "id": "Al5CiT161LDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def infer_translation(model, tokenizer, device, input_text, max_length=20):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Tokenize the user input\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "\n",
        "        # Generate translation\n",
        "        outputs = model(input_ids, max_length=max_length)\n",
        "        outputs = outputs.argmax(-1)\n",
        "\n",
        "        # Decode the predicted tokens to a human-readable string\n",
        "        predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return predicted_text\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Assuming the model, tokenizer, and device are already defined\n",
        "    while True:\n",
        "        input_text = input(\"Enter text to translate (or 'exit' to quit): \")\n",
        "        if input_text.lower() == 'exit':\n",
        "            break\n",
        "        translation = infer_translation(model, tokenizer, device, input_text)\n",
        "        print(f\"Translation: {translation}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YZxTDB704Ih",
        "outputId": "7ede3ed0-b182-44a0-f602-1fd14e98f3cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter text to translate (or 'exit' to quit): hello world\n",
            "Translation: \n",
            "Enter text to translate (or 'exit' to quit): hi can you see i t\n",
            "Translation: kan jag t\n",
            "Enter text to translate (or 'exit' to quit): my name is this\n",
            "Translation: det här s\n",
            "Enter text to translate (or 'exit' to quit): this is my dog\n",
            "Translation: detta är mint\n",
            "Enter text to translate (or 'exit' to quit): this is the european union\n",
            "Translation: unionen unionen etabler\n",
            "Enter text to translate (or 'exit' to quit): let's start this meeting\n",
            "Translation: det börjar\n",
            "Enter text to translate (or 'exit' to quit): mr president I want to have your attention\n",
            "Translation: herr talman för att g\n",
            "Enter text to translate (or 'exit' to quit): I want to talk about this subject\n",
            "Translation: det här sätt\n",
            "Enter text to translate (or 'exit' to quit): I want to talk about european parliament\n",
            "Translation: för att ått\n",
            "Enter text to translate (or 'exit' to quit): this is a meeting \n",
            "Translation: det här en\n",
            "Enter text to translate (or 'exit' to quit): this is a nice day\n",
            "Translation: det är entt\n",
            "Enter text to translate (or 'exit' to quit): this is a short sentence\n",
            "Translation: det är ent so\n",
            "Enter text to translate (or 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. T5 Arhitecture"
      ],
      "metadata": {
        "id": "dUhBbGdf90e2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-sv')\n",
        "\n",
        "modelt5 = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "modelt5.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "#modelt5_path1 =torch.load('drive/MyDrive/s_models/t5-model.pth')\n",
        "modelt5_path = torch.load('drive/MyDrive/s_models/t5-model.pth',map_location=torch.device('cpu')) # This part needs to be changed also\n",
        "\n",
        "modelt5.load_state_dict(modelt5_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sz78XEC22RB",
        "outputId": "73226ebe-7f3d-4de6-c715-4e9e666285b1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def infer_translation2(model, tokenizer, device, input_text, max_length=20):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        # Tokenize the input_text and ensure it's in the right format\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors='pt')  # Tokenize and convert to tensor\n",
        "        input_ids = input_ids.to(device)  # Move to the appropriate device\n",
        "\n",
        "        # Generate output using the model\n",
        "        outputs = model.generate(input_ids=input_ids, max_length=max_length, num_beams=1, early_stopping=False)\n",
        "        predicted_sentences = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        predictions.extend(predicted_sentences)\n",
        "        #print(predicted_sentences)\n",
        "\n",
        "    return predicted_sentences\n"
      ],
      "metadata": {
        "id": "ovpZivz59zwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cpu')\n",
        "print ('Device set to {0}'.format(device))\n",
        "\n",
        "while True:\n",
        "  input_text = input(\"Enter text to translate (or 'exit' to quit): \")\n",
        "  if input_text.lower() == 'exit':\n",
        "    break\n",
        "  translation2 = infer_translation2(modelt5, tokenizer, device, input_text)\n",
        "  print(f\"Translation: {translation2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr1rLr-8_uAR",
        "outputId": "2b94f93f-74df-4fe6-bffe-6025ee332758"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device set to cpu\n",
            "Enter text to translate (or 'exit' to quit): this is the beginning of the session\n",
            "Translation: ['detta är början på sessionen det är början på sessionen det är början på den sessionen']\n",
            "Enter text to translate (or 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Minor Note"
      ],
      "metadata": {
        "id": "P8PZ9berTFi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In these models, we can observe that the sequence to sequence model gives some results where usually the start of the sentence matches. The T5 model, even though it gives much more accurate translation, however seems to suffer from an issue whereby it repeats the translation more than one times. Things that are worth investigating further...\n",
        "\n",
        "- Have fun with playing with english to swedish translation"
      ],
      "metadata": {
        "id": "oyFxAChMTLTo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LCI2r-LfTKmP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}